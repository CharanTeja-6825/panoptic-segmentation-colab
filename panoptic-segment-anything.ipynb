{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharanTeja-6825/panoptic-segmentation-colab/blob/main/panoptic-segment-anything.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfN3e_6aUnDe"
      },
      "source": [
        "# Panoptic SAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5a6aCnoAtnR"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ExRSgV7bPc0"
      },
      "source": [
        "Install the required libraries + code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riHnEYErHS5c",
        "outputId": "08eee473-c54b-456a-8b1d-05f311eb3854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n",
            "/content/Grounded-Segment-Anything\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.4/217.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding editable for groundingdino \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building editable for groundingdino (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building editable for groundingdino\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (groundingdino)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate scipy safetensors segments-ai\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "!pip install -q -e /content/Grounded-Segment-Anything/GroundingDINO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62FL0vOmbWg7"
      },
      "source": [
        "Import the libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-fVmeYEyMp4Q"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "sys.path.append(\n",
        "    os.path.join(os.getcwd(), \"/content/Grounded-Segment-Anything/GroundingDINO\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WjP3qi_4EeWR",
        "outputId": "1fdf08eb-97b8-44c5-9d0b-8c2b189db74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'addict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1101898148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Grounding DINO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroundingDINO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mGroundingDINO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mGroundingDINO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbox_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mGroundingDINO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mGroundingDINO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_groundingdino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# ------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgroundingdino\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_groundingdino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Grounded-Segment-Anything/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnested_tensor_from_tensor_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_phrases_from_posmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCOVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvl_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_positive_map_from_span\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Grounded-Segment-Anything/GroundingDINO/groundingdino/util/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Grounded-Segment-Anything/GroundingDINO/groundingdino/util/slconfig.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maddict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myapf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myapflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myapf_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFormatCode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'addict'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import random\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from scipy import ndimage\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "from segments import SegmentsClient\n",
        "from segments.export import colorize\n",
        "from segments.utils import bitmap2file\n",
        "from getpass import getpass\n",
        "\n",
        "# Grounding DINO\n",
        "import GroundingDINO.groundingdino.datasets.transforms as T\n",
        "from GroundingDINO.groundingdino.models import build_model\n",
        "from GroundingDINO.groundingdino.util import box_ops\n",
        "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
        "from GroundingDINO.groundingdino.util.utils import clean_state_dict\n",
        "from GroundingDINO.groundingdino.util.inference import annotate, predict\n",
        "\n",
        "# segment anything\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "\n",
        "# CLIPSeg\n",
        "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqYTsptAj8B5"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "if device != \"cpu\":\n",
        "    try:\n",
        "        from GroundingDINO.groundingdino import _C\n",
        "    except:\n",
        "        warnings.warn(\n",
        "            \"Failed to load custom C++ ops. Running on CPU mode Only in groundingdino!\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90hAtFtvbe4f"
      },
      "source": [
        "Download + load the Grounding DINO model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZmIm1pxNFuk"
      },
      "outputs": [],
      "source": [
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    model = build_model(args)\n",
        "    args.device = device\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location=\"cpu\")\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "    _ = model.eval()\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Use this command for evaluate the Grounding DINO model\n",
        "# Or you can download the model by yourself\n",
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n",
        "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "\n",
        "groundingdino_model = load_model_hf(\n",
        "    ckpt_repo_id, ckpt_filename, ckpt_config_filename, device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4211Ov4dbhWt"
      },
      "source": [
        "Download SAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4v5H0heNVz3"
      },
      "outputs": [],
      "source": [
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8tcx6dYbnC8"
      },
      "source": [
        "Load SAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqa_jMGlNW_r"
      },
      "outputs": [],
      "source": [
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "sam = build_sam(checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "sam_predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAMTxbQRboy-"
      },
      "source": [
        "Download + load CLIPSeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCdgxg4SXg_r"
      },
      "outputs": [],
      "source": [
        "clipseg_processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
        "clipseg_model = CLIPSegForImageSegmentation.from_pretrained(\n",
        "    \"CIDAS/clipseg-rd64-refined\"\n",
        ")\n",
        "clipseg_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5viuQFdbA0Ha"
      },
      "source": [
        "## Load dataset from Segments.ai (optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAtvgwo7Sj8"
      },
      "source": [
        "First, create an account at [https://segments.ai/join](https://segments.ai/join?utm_source=hf&utm_medium=colab&utm_campaign=clipseg). Then you can initialize the Segments.ai Python client using your API key, which can be found on [your account page](https://segments.ai/account).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWwGgvJ-7yZR"
      },
      "outputs": [],
      "source": [
        "api_key = getpass(\"Enter your API key: \")\n",
        "segments_client = SegmentsClient(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6WZ89IA9jgw"
      },
      "source": [
        "Next, let's load the [a2d2 self-driving dataset](https://www.a2d2.audi/a2d2/en.html) using the Segments client. You can also create your own dataset by following [these instructions](https://docs.segments.ai/tutorials/getting-started).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmEl1Vjd-Sh1"
      },
      "outputs": [],
      "source": [
        "dataset_identifier = \"admin-tobias/panoptic-sam\"\n",
        "\n",
        "samples = segments_client.get_samples(dataset_identifier)\n",
        "dataset = segments_client.get_dataset(dataset_identifier)\n",
        "categories = dataset.task_attributes.categories\n",
        "category_name_to_id = {category.name: category.id for category in categories}\n",
        "\n",
        "# split the categories into \"stuff\" categories (regions w/o instances)\n",
        "# and \"thing\" categories (objects/instances)\n",
        "stuff_categories = [category for category in categories if not category.has_instances]\n",
        "thing_categories = [category for category in categories if category.has_instances]\n",
        "stuff_category_names = [category.name for category in stuff_categories]\n",
        "thing_category_names = [category.name for category in thing_categories]\n",
        "\n",
        "print(\"Stuff categories\", stuff_category_names)\n",
        "print(\"Thing categories\", thing_category_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsvW-detA9ze"
      },
      "source": [
        "## Panoptic SAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Ytwxx8QeIZ"
      },
      "source": [
        "### Helper methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VuoDDJ4QYry"
      },
      "outputs": [],
      "source": [
        "def download_image(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "\n",
        "def load_image_for_dino(image):\n",
        "    transform = T.Compose(\n",
        "        [\n",
        "            T.RandomResize([800], max_size=1333),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "    dino_image, _ = transform(image, None)\n",
        "    return dino_image\n",
        "\n",
        "\n",
        "def dino_detection(\n",
        "    model,\n",
        "    image,\n",
        "    image_array,\n",
        "    category_names,\n",
        "    category_name_to_id,\n",
        "    box_threshold,\n",
        "    text_threshold,\n",
        "    device,\n",
        "    visualize=False,\n",
        "):\n",
        "    detection_prompt = \" . \".join(category_names)\n",
        "    dino_image = load_image_for_dino(image)\n",
        "    dino_image = dino_image.to(device)\n",
        "    with torch.no_grad():\n",
        "        boxes, logits, phrases = predict(\n",
        "            model=model,\n",
        "            image=dino_image,\n",
        "            caption=detection_prompt,\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold,\n",
        "            device=device,\n",
        "            remove_combined=True,\n",
        "        )\n",
        "    category_ids = [category_name_to_id[phrase] for phrase in phrases]\n",
        "\n",
        "    if visualize:\n",
        "        annotated_frame = annotate(\n",
        "            image_source=image_array, boxes=boxes, logits=logits, phrases=phrases\n",
        "        )\n",
        "        annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
        "        visualization = Image.fromarray(annotated_frame)\n",
        "        return boxes, category_ids, visualization\n",
        "    else:\n",
        "        return boxes, category_ids, phrases\n",
        "\n",
        "\n",
        "def sam_masks_from_dino_boxes(predictor, image_array, boxes, device):\n",
        "    # box: normalized box xywh -> unnormalized xyxy\n",
        "    H, W, _ = image_array.shape\n",
        "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "    transformed_boxes = predictor.transform.apply_boxes_torch(\n",
        "        boxes_xyxy, image_array.shape[:2]\n",
        "    ).to(device)\n",
        "    thing_masks, _, _ = predictor.predict_torch(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        boxes=transformed_boxes,\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    return thing_masks\n",
        "\n",
        "\n",
        "def preds_to_semantic_inds(preds, threshold):\n",
        "    flat_preds = preds.reshape((preds.shape[0], -1))\n",
        "    # Initialize a dummy \"unlabeled\" mask with the threshold\n",
        "    flat_preds_with_treshold = torch.full(\n",
        "        (preds.shape[0] + 1, flat_preds.shape[-1]), threshold\n",
        "    )\n",
        "    flat_preds_with_treshold[1 : preds.shape[0] + 1, :] = flat_preds\n",
        "\n",
        "    # Get the top mask index for each pixel\n",
        "    semantic_inds = torch.topk(flat_preds_with_treshold, 1, dim=0).indices.reshape(\n",
        "        (preds.shape[-2], preds.shape[-1])\n",
        "    )\n",
        "\n",
        "    return semantic_inds\n",
        "\n",
        "\n",
        "def clipseg_segmentation(\n",
        "    processor, model, image, category_names, background_threshold, device\n",
        "):\n",
        "    inputs = processor(\n",
        "        text=category_names,\n",
        "        images=[image] * len(category_names),\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    if len(logits.shape) == 2:\n",
        "      logits = logits.unsqueeze(0)\n",
        "    # resize the outputs\n",
        "    upscaled_logits = nn.functional.interpolate(\n",
        "        logits.unsqueeze(1),\n",
        "        size=(image.size[1], image.size[0]),\n",
        "        mode=\"bilinear\",\n",
        "    )\n",
        "    preds = torch.sigmoid(upscaled_logits.squeeze(dim=1))\n",
        "    semantic_inds = preds_to_semantic_inds(preds, background_threshold)\n",
        "    return preds, semantic_inds\n",
        "\n",
        "\n",
        "def semantic_inds_to_shrunken_bool_masks(\n",
        "    semantic_inds, shrink_kernel_size, num_categories\n",
        "):\n",
        "    shrink_kernel = np.ones((shrink_kernel_size, shrink_kernel_size))\n",
        "\n",
        "    bool_masks = torch.zeros((num_categories, *semantic_inds.shape), dtype=bool)\n",
        "    for category in range(num_categories):\n",
        "        binary_mask = semantic_inds == category\n",
        "        shrunken_binary_mask_array = (\n",
        "            ndimage.binary_erosion(binary_mask.numpy(), structure=shrink_kernel)\n",
        "            if shrink_kernel_size > 0\n",
        "            else binary_mask.numpy()\n",
        "        )\n",
        "        bool_masks[category] = torch.from_numpy(shrunken_binary_mask_array)\n",
        "\n",
        "    return bool_masks\n",
        "\n",
        "\n",
        "def clip_and_shrink_preds(semantic_inds, preds, shrink_kernel_size, num_categories):\n",
        "    # convert semantic_inds to shrunken bool masks\n",
        "    bool_masks = semantic_inds_to_shrunken_bool_masks(\n",
        "        semantic_inds, shrink_kernel_size, num_categories\n",
        "    ).to(preds.device)\n",
        "\n",
        "    sizes = [\n",
        "        torch.sum(bool_masks[i].int()).item() for i in range(1, bool_masks.size(0))\n",
        "    ]\n",
        "    max_size = max(sizes)\n",
        "    relative_sizes = [size / max_size for size in sizes] if max_size > 0 else sizes\n",
        "\n",
        "    # use bool masks to clip preds\n",
        "    clipped_preds = torch.zeros_like(preds)\n",
        "    for i in range(1, bool_masks.size(0)):\n",
        "        float_mask = bool_masks[i].float()\n",
        "        clipped_preds[i - 1] = preds[i - 1] * float_mask\n",
        "\n",
        "    return clipped_preds, relative_sizes\n",
        "\n",
        "\n",
        "def sample_points_based_on_preds(preds, N):\n",
        "    height, width = preds.shape\n",
        "    weights = preds.ravel()\n",
        "    indices = np.arange(height * width)\n",
        "\n",
        "    # Randomly sample N indices based on the weights\n",
        "    sampled_indices = random.choices(indices, weights=weights, k=N)\n",
        "\n",
        "    # Convert the sampled indices into (col, row) coordinates\n",
        "    sampled_points = [(index % width, index // width) for index in sampled_indices]\n",
        "\n",
        "    return sampled_points\n",
        "\n",
        "\n",
        "def upsample_pred(pred, image_source):\n",
        "    pred = pred.unsqueeze(dim=0)\n",
        "    original_height = image_source.shape[0]\n",
        "    original_width = image_source.shape[1]\n",
        "\n",
        "    larger_dim = max(original_height, original_width)\n",
        "    aspect_ratio = original_height / original_width\n",
        "\n",
        "    # upsample the tensor to the larger dimension\n",
        "    upsampled_tensor = F.interpolate(\n",
        "        pred, size=(larger_dim, larger_dim), mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "\n",
        "    # remove the padding (at the end) to get the original image resolution\n",
        "    if original_height > original_width:\n",
        "        target_width = int(upsampled_tensor.shape[3] * aspect_ratio)\n",
        "        upsampled_tensor = upsampled_tensor[:, :, :, :target_width]\n",
        "    else:\n",
        "        target_height = int(upsampled_tensor.shape[2] * aspect_ratio)\n",
        "        upsampled_tensor = upsampled_tensor[:, :, :target_height, :]\n",
        "    return upsampled_tensor.squeeze(dim=1)\n",
        "\n",
        "\n",
        "def sam_mask_from_points(predictor, image_array, points):\n",
        "    points_array = np.array(points)\n",
        "    # we only sample positive points, so labels are all 1\n",
        "    points_labels = np.ones(len(points))\n",
        "    # we don't use predict_torch here cause it didn't seem to work...\n",
        "    masks, scores, logits = predictor.predict(\n",
        "        point_coords=points_array,\n",
        "        point_labels=points_labels,\n",
        "    )\n",
        "    # max over the 3 segmentation levels\n",
        "    total_pred = torch.max(torch.sigmoid(torch.tensor(logits)), dim=0)[0].unsqueeze(\n",
        "        dim=0\n",
        "    )\n",
        "    # logits are 256x256 -> upsample back to image shape\n",
        "    upsampled_pred = upsample_pred(total_pred, image_array)\n",
        "    return upsampled_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUkQZ-ULQiRd"
      },
      "source": [
        "### Panoptic SAM pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXHlPBCUUL3H"
      },
      "source": [
        "The next method defines the full pipeline for a single image input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNPQrTlQOVj"
      },
      "outputs": [],
      "source": [
        "def generate_panoptic_mask(\n",
        "    image,\n",
        "    thing_category_names,\n",
        "    stuff_category_names,\n",
        "    category_name_to_id,\n",
        "    dino_model,\n",
        "    sam_predictor,\n",
        "    clipseg_processor,\n",
        "    clipseg_model,\n",
        "    device,\n",
        "    dino_box_threshold=0.3,\n",
        "    dino_text_threshold=0.25,\n",
        "    segmentation_background_threshold=0.1,\n",
        "    shrink_kernel_size=20,\n",
        "    num_samples_factor=1000,\n",
        "):\n",
        "    image = image.convert(\"RGB\")\n",
        "    image_array = np.asarray(image)\n",
        "\n",
        "    # compute SAM image embedding\n",
        "    sam_predictor.set_image(image_array)\n",
        "\n",
        "    # detect boxes for \"thing\" categories using Grounding DINO\n",
        "    thing_category_ids = []\n",
        "    thing_masks = []\n",
        "    thing_boxes = []\n",
        "    if len(thing_category_names) > 0:\n",
        "        thing_boxes, thing_category_ids, _ = dino_detection(\n",
        "            dino_model,\n",
        "            image,\n",
        "            image_array,\n",
        "            thing_category_names,\n",
        "            category_name_to_id,\n",
        "            dino_box_threshold,\n",
        "            dino_text_threshold,\n",
        "            device,\n",
        "        )\n",
        "        if len(thing_boxes) > 0:\n",
        "            # get segmentation masks for the thing boxes\n",
        "            thing_masks = sam_masks_from_dino_boxes(\n",
        "                sam_predictor, image_array, thing_boxes, device\n",
        "            )\n",
        "\n",
        "    if len(stuff_category_names) > 0:\n",
        "        # get rough segmentation masks for \"stuff\" categories using CLIPSeg\n",
        "        clipseg_preds, clipseg_semantic_inds = clipseg_segmentation(\n",
        "            clipseg_processor,\n",
        "            clipseg_model,\n",
        "            image,\n",
        "            stuff_category_names,\n",
        "            segmentation_background_threshold,\n",
        "            device,\n",
        "        )\n",
        "        # remove things from stuff masks\n",
        "        clipseg_semantic_inds_without_things = clipseg_semantic_inds.clone()\n",
        "        if len(thing_boxes) > 0:\n",
        "            combined_things_mask = torch.any(thing_masks, dim=0)\n",
        "            clipseg_semantic_inds_without_things[combined_things_mask[0]] = 0\n",
        "        # clip CLIPSeg preds based on non-overlapping semantic segmentation inds (+ optionally shrink the mask of each category)\n",
        "        # also returns the relative size of each category\n",
        "        clipsed_clipped_preds, relative_sizes = clip_and_shrink_preds(\n",
        "            clipseg_semantic_inds_without_things,\n",
        "            clipseg_preds,\n",
        "            shrink_kernel_size,\n",
        "            len(stuff_category_names) + 1,\n",
        "        )\n",
        "        # get finer segmentation masks for the \"stuff\" categories using SAM\n",
        "        sam_preds = torch.zeros_like(clipsed_clipped_preds)\n",
        "        for i in range(clipsed_clipped_preds.shape[0]):\n",
        "            clipseg_pred = clipsed_clipped_preds[i]\n",
        "            # for each \"stuff\" category, sample points in the rough segmentation mask\n",
        "            num_samples = int(relative_sizes[i] * num_samples_factor)\n",
        "            if num_samples == 0:\n",
        "                continue\n",
        "            points = sample_points_based_on_preds(\n",
        "                clipseg_pred.cpu().numpy(), num_samples\n",
        "            )\n",
        "            if len(points) == 0:\n",
        "                continue\n",
        "            # use SAM to get mask for points\n",
        "            pred = sam_mask_from_points(sam_predictor, image_array, points)\n",
        "            sam_preds[i] = pred\n",
        "        sam_semantic_inds = preds_to_semantic_inds(\n",
        "            sam_preds, segmentation_background_threshold\n",
        "        )\n",
        "\n",
        "    # combine the thing inds and the stuff inds into panoptic inds\n",
        "    panoptic_inds = (\n",
        "        sam_semantic_inds.clone()\n",
        "        if len(stuff_category_names) > 0\n",
        "        else torch.zeros(image_array.shape[0], image_array.shape[1], dtype=torch.long)\n",
        "    )\n",
        "    ind = len(stuff_category_names) + 1\n",
        "    for thing_mask in thing_masks:\n",
        "        # overlay thing mask on panoptic inds\n",
        "        panoptic_inds[thing_mask.squeeze(dim=0)] = ind\n",
        "        ind += 1\n",
        "\n",
        "    return panoptic_inds, thing_category_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVgDVHIa84o"
      },
      "source": [
        "Let's download an image and try it out!\n",
        "Uncomment and run the second cell if you didn't load a dataset from Segments.ai.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bEu7gS-j8B7"
      },
      "outputs": [],
      "source": [
        "sample = samples[0]\n",
        "image_url = sample.attributes.image.url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfHmivs_j8B7"
      },
      "outputs": [],
      "source": [
        "# image_url = \"https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/admin-tobias/996f3571-66d8-4dfd-8c1a-402d7c0c820b.png\"\n",
        "# thing_category_names = [\"car\", \"person\", \"bus\"]\n",
        "# stuff_category_names = [\"building\", \"road\", \"sky\", \"trees\", \"sidewalk\"]\n",
        "# category_names = thing_category_names + stuff_category_names\n",
        "# category_name_to_id = {\n",
        "#     category_name: i for i, category_name in enumerate(category_names)\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCUZLWxamPcr"
      },
      "outputs": [],
      "source": [
        "image = download_image(image_url)\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjyYeQiSp6HX"
      },
      "source": [
        "Running the whole pipeline takes less than 10 seconds on Google Colab (in a GPU runtime). On CPU, it takes around 2 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH9FJ6D2mBVA"
      },
      "outputs": [],
      "source": [
        "panoptic_inds, thing_category_ids = generate_panoptic_mask(\n",
        "    image,\n",
        "    thing_category_names,\n",
        "    stuff_category_names,\n",
        "    category_name_to_id,\n",
        "    groundingdino_model,\n",
        "    sam_predictor,\n",
        "    clipseg_processor,\n",
        "    clipseg_model,\n",
        "    device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGgbqrDVwQuH"
      },
      "source": [
        "We can visualize the panoptic segmentation result by colorizing the masks and overlaying them on the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GJhWXaqsTu4"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax.imshow(image)\n",
        "ax.imshow(colorize(panoptic_inds), alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5qoXhvgN9ax"
      },
      "source": [
        "Lastly, we can upload the prediction to Segments.ai. To do that, we'll first convert the `panoptic_inds` tensor to a png file, then we'll upload this file to the Segments, and finally we'll add the label to the sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vUihdVtG9KR"
      },
      "outputs": [],
      "source": [
        "def inds_to_segments_format(\n",
        "    panoptic_inds, thing_category_ids, stuff_category_names, category_name_to_id\n",
        "):\n",
        "    panoptic_inds_array = panoptic_inds.numpy().astype(np.uint32)\n",
        "    bitmap_file = bitmap2file(panoptic_inds_array, is_segmentation_bitmap=True)\n",
        "\n",
        "    stuff_category_ids = [\n",
        "        category_name_to_id[stuff_category_name]\n",
        "        for stuff_category_name in stuff_category_names\n",
        "    ]\n",
        "\n",
        "    unique_inds = np.unique(panoptic_inds_array)\n",
        "    stuff_annotations = [\n",
        "        {\"id\": i, \"category_id\": stuff_category_ids[i - 1]}\n",
        "        for i in range(1, len(stuff_category_names) + 1)\n",
        "        if i in unique_inds\n",
        "    ]\n",
        "    thing_annotations = [\n",
        "        {\"id\": len(stuff_category_names) + 1 + i, \"category_id\": thing_category_id}\n",
        "        for i, thing_category_id in enumerate(thing_category_ids)\n",
        "    ]\n",
        "    annotations = stuff_annotations + thing_annotations\n",
        "\n",
        "    return bitmap_file, annotations\n",
        "\n",
        "\n",
        "def upload_label(\n",
        "    panoptic_inds,\n",
        "    thing_category_ids,\n",
        "    stuff_category_names,\n",
        "    category_name_to_id,\n",
        "    segments_client,\n",
        "    sample,\n",
        "):\n",
        "    bitmap_file, annotations = inds_to_segments_format(\n",
        "        panoptic_inds, thing_category_ids, stuff_category_names, category_name_to_id\n",
        "    )\n",
        "\n",
        "    asset = segments_client.upload_asset(bitmap_file, \"panoptic_sam_prediction.png\")\n",
        "\n",
        "    attributes = {\n",
        "        \"format_version\": \"0.1\",\n",
        "        \"annotations\": annotations,\n",
        "        \"segmentation_bitmap\": {\"url\": asset.url},\n",
        "    }\n",
        "\n",
        "    segments_client.add_label(sample.uuid, \"ground-truth\", attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FRs_G3vupfq"
      },
      "outputs": [],
      "source": [
        "upload_label(\n",
        "    panoptic_inds,\n",
        "    thing_category_ids,\n",
        "    stuff_category_names,\n",
        "    category_name_to_id,\n",
        "    segments_client,\n",
        "    sample,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJKlyVkKxcXu"
      },
      "source": [
        "## Run Panoptic SAM on the whole dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S83dMotmxrVP"
      },
      "outputs": [],
      "source": [
        "for sample in samples:\n",
        "    image = download_image(sample.attributes.image.url)\n",
        "    panoptic_inds, thing_category_ids = generate_panoptic_mask(\n",
        "        image,\n",
        "        thing_category_names,\n",
        "        stuff_category_names,\n",
        "        category_name_to_id,\n",
        "        groundingdino_model,\n",
        "        sam_predictor,\n",
        "        clipseg_processor,\n",
        "        clipseg_model,\n",
        "        device,\n",
        "    )\n",
        "    upload_label(\n",
        "        panoptic_inds,\n",
        "        thing_category_ids,\n",
        "        stuff_category_names,\n",
        "        category_name_to_id,\n",
        "        segments_client,\n",
        "        sample,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-L7Fav4BDJ7"
      },
      "source": [
        "## Visualizations of the different pipeline steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZTxtkRhU5s6"
      },
      "source": [
        "Let's first define some visualization methods to help us see what's going on in the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtUobcPPoG6"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_segmentation_preds(preds, category_names):\n",
        "    len_cats = len(category_names)\n",
        "    _, ax = plt.subplots(1, len_cats + 1, figsize=(3 * (len_cats + 1), 4))\n",
        "    [a.axis(\"off\") for a in ax.flatten()]\n",
        "    ax[0].imshow(image)\n",
        "    [ax[i + 1].imshow(preds[i].cpu()) for i in range(len_cats)]\n",
        "    [\n",
        "        ax[i + 1].text(0, -15, category_name)\n",
        "        for i, category_name in enumerate(category_names)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-PMwm-1qfz"
      },
      "source": [
        "Input image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmDARihVz7xy"
      },
      "outputs": [],
      "source": [
        "image = image.convert(\"RGB\")\n",
        "image_array = np.asarray(image)\n",
        "plt.imshow(image_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfh-ftkg1uP5"
      },
      "source": [
        "Boxes for \"thing\" categories from Grounding DINO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gufSd2rDztTK"
      },
      "outputs": [],
      "source": [
        "dino_box_threshold = 0.3\n",
        "dino_text_threshold = 0.25\n",
        "\n",
        "thing_boxes, thing_category_ids, visualization = dino_detection(\n",
        "    groundingdino_model,\n",
        "    image,\n",
        "    image_array,\n",
        "    thing_category_names,\n",
        "    category_name_to_id,\n",
        "    dino_box_threshold,\n",
        "    dino_text_threshold,\n",
        "    device,\n",
        "    visualize=True,\n",
        ")\n",
        "visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3n3rRUW17Nv"
      },
      "source": [
        "Segmentation masks for the thing boxes from SAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI4ef2wI0XRK"
      },
      "outputs": [],
      "source": [
        "# compute SAM image embedding\n",
        "sam_predictor.set_image(image_array)\n",
        "thing_masks = sam_masks_from_dino_boxes(sam_predictor, image_array, thing_boxes, device)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "for thing_mask in thing_masks:\n",
        "    show_mask(thing_mask.cpu().numpy(), plt.gca(), random_color=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI0gARK_2OoM"
      },
      "source": [
        "CLIPSeg predictions for the \"stuff\" categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orffeyYnPql0"
      },
      "outputs": [],
      "source": [
        "segmentation_background_threshold = 0.1\n",
        "\n",
        "# get rough segmentation masks for \"stuff\" categories using CLIPSeg\n",
        "clipseg_preds, clipseg_semantic_inds = clipseg_segmentation(\n",
        "    clipseg_processor,\n",
        "    clipseg_model,\n",
        "    image,\n",
        "    stuff_category_names,\n",
        "    segmentation_background_threshold,\n",
        "    device,\n",
        ")\n",
        "show_segmentation_preds(clipseg_preds, stuff_category_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi7BvMoF2eLR"
      },
      "source": [
        "Integrated semantic segmentation output from CLIPSeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNH7goQ92bSd"
      },
      "outputs": [],
      "source": [
        "plt.imshow(clipseg_semantic_inds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnR9gsh82kAw"
      },
      "source": [
        "Clipped and shrunken CLIPSeg preds based on the integrated segmentation map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYuwssgxrOhm"
      },
      "outputs": [],
      "source": [
        "# remove things from stuff masks\n",
        "clipseg_semantic_inds_without_things = clipseg_semantic_inds.clone()\n",
        "if len(thing_boxes > 0):\n",
        "    combined_things_mask = torch.any(thing_masks, dim=0)\n",
        "    clipseg_semantic_inds_without_things[combined_things_mask[0]] = 0\n",
        "# clip CLIPSeg preds based on non-overlapping semantic segmentation inds (+ optionally shrink the mask of each category)\n",
        "# also returns the relative size of each category\n",
        "shrink_kernel_size = 20\n",
        "clipsed_clipped_preds, relative_sizes = clip_and_shrink_preds(\n",
        "    clipseg_semantic_inds_without_things,\n",
        "    clipseg_preds,\n",
        "    shrink_kernel_size,\n",
        "    len(stuff_category_names) + 1,\n",
        ")\n",
        "show_segmentation_preds(clipsed_clipped_preds, stuff_category_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjoQkYG13TER"
      },
      "source": [
        "Let's use SAM to refine the segmentation mask of one of the categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gue97YA2Imv"
      },
      "outputs": [],
      "source": [
        "i = 1\n",
        "clipseg_pred = clipsed_clipped_preds[i]\n",
        "# for each \"stuff\" category, sample points in the rough segmentation mask\n",
        "num_samples_factor = 1000\n",
        "num_samples = int(relative_sizes[i] * num_samples_factor)\n",
        "points = sample_points_based_on_preds(clipseg_pred.cpu().numpy(), num_samples)\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "ax.imshow(image)\n",
        "x_coords, y_coords = zip(*points)\n",
        "ax.scatter(x_coords, y_coords, color=\"red\", marker=\"x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N0CorwmCvHj"
      },
      "outputs": [],
      "source": [
        "# use SAM to get mask for points\n",
        "pred = sam_mask_from_points(sam_predictor, image_array, points)\n",
        "plt.imshow(pred.squeeze(dim=0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_5a6aCnoAtnR",
        "5viuQFdbA0Ha",
        "r8Ytwxx8QeIZ",
        "UJKlyVkKxcXu",
        "U-L7Fav4BDJ7"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.0 (default, Feb 23 2022, 10:45:45) \n[GCC 9.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c1330ff811d49988c6bd3f6d30257d79d8234a7d606d1a91e87e277837a80053"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}